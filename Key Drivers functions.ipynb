{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a238165c-5d6b-49a9-9969-c0368d11b8ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import key libraries\n",
    "import pyspark\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime as dt\n",
    "import pytz \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import model_selection \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.system(\"\")\n",
    "class style():\n",
    "    RED = '\\033[31m'\n",
    "    \n",
    "# site and machine dictionary\n",
    "# @Glori, please updatet the site_machine_dict when our data from other machine comes in: \n",
    "\n",
    "site_machine_dict = {'ay':['1a','2a','3a','4a','5a','6a'],\n",
    "                    'beu':['15b'],\n",
    "                   'cg':['5g','6g','7g'],\n",
    "                   'gb':['10f','11f','12f','13f','14f','15f'],\n",
    "                   'mp':['1m','2m','3m','4m','5m','6m','7m','8m'],\n",
    "                   'ox':['1x','2x']}\n",
    "\n",
    "# Convert dictionary to dataframe \n",
    "site_machine_df = pd.DataFrame (((i,j)for i in site_machine_dict.keys() for j in site_machine_dict[i]), columns = ['site','machines'])\n",
    "\n",
    "# machine leanring model dictionary\n",
    "model_dict={'LinearRegression':LinearRegression(),'Ridge':Ridge(),'Lasso':Lasso(),'RandomForestRegressor':RandomForestRegressor()}\n",
    "\n",
    "def creat_sql(select_site, select_machine):\n",
    "    \"\"\"\n",
    "    Function that create sql that allow users to interact with database\n",
    "    Parameters:\n",
    "    select_site: str - Site name user selected\n",
    "    select_machine: str - Machine name user selected\n",
    "    \"\"\"\n",
    "    # build machine_table \n",
    "    machine_table = '_'.join([select_site,select_machine,'timeseries'])\n",
    "    # build data_table\n",
    "    data_table = '.'.join(['groupdb_famc_energy_analytics',machine_table])\n",
    "    # build sql \n",
    "    sql = 'select * from ' +data_table\n",
    "    return sql\n",
    "\n",
    "def create_widget(name, data, feature, type):\n",
    "    \"\"\"\n",
    "    Function that creates widges in the notebook that allow users interact with the code\n",
    "    Parameters:\n",
    "    name: str - Name desired for the widget\n",
    "    data: pd.DataFrame - dataframe containing the data needed to create the widget\n",
    "    feature: str - name of the column to be used to extract the unique values from data\n",
    "    \"\"\"\n",
    "    items = data[feature].unique()\n",
    "    if type == 'dropdown':\n",
    "        dbutils.widgets.dropdown(name, items[0], [x for x in items])\n",
    "    elif type == 'multiselect':\n",
    "        dbutils.widgets.multiselect(name, items[0], [x for x in items])\n",
    "    else:\n",
    "        print('Widget type not recognized')\n",
    "\n",
    "def historical_data (data, n_months):\n",
    "    \"\"\"\n",
    "    Helper function to get the historical data\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed to retrieve historical data\n",
    "    n_months: int - Number of months the user can track from historoical data, defaulted to be the last 18 months\n",
    "    \"\"\"\n",
    "    today = date.today()\n",
    "    \n",
    "    #past_date  = today - pd.DateOffset(months=n_months)\n",
    "    past_date  = today - pd.DateOffset(months=n_months)\n",
    "    \n",
    "    data = data[data['time_bucket_local'] >= str(past_date)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prod_codes_sorting(data):\n",
    "    \"\"\"\n",
    "    Helper function that sort the prod_codes of one machine by Descending order via number of datapoints\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed to retrieve prod_codes\n",
    "    \"\"\" \n",
    "    prod_code = data.groupby('prod_code')['time_bucket_local'].count().reset_index().sort_values(by='time_bucket_local',ascending = False)\n",
    "    return prod_code\n",
    "\n",
    "def get_time_series_data (data,select_prod_codes):\n",
    "    \"\"\"\n",
    "    Function that create training data based on the prod_codes that have been selected\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed to create training data\n",
    "    \"\"\" \n",
    "     # filter to only selected prod_codes\n",
    "    data = data [data['prod_code'].isin(select_prod_codes)] \n",
    "    \n",
    "    # get rid of redundant columns \n",
    "    time_series_data = data.iloc[:,2:]\n",
    "    time_series_data.drop(['machine','time_bucket_utc'],axis = 1,inplace = True)\n",
    "    \n",
    "    return time_series_data   \n",
    "\n",
    "def get_prod_data (data,select_prod_codes):\n",
    "    \"\"\"\n",
    "    Function that create training data based on the prod_codes that have been selected\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed to create training data\n",
    "    select_prod_codes: list - list conatining the select_prod_codes \n",
    "    \"\"\"\n",
    "    # filter to only selected prod_codes\n",
    "    data = data [data['prod_code'].isin(select_prod_codes)]  \n",
    "    \n",
    "    return data  \n",
    "\n",
    "def remove_outlier(df):\n",
    "    df = df.dropna()\n",
    "    # define outlier function for each column \n",
    "    def outliers(df, ft): \n",
    "        \n",
    "        lower_bound = df[ft].quantile(0.05)\n",
    "        upper_bound = df[ft].quantile(0.99)\n",
    "        \n",
    "        ft_index = df.index[(df[ft]<lower_bound)|(df[ft]>upper_bound)]\n",
    "        return ft_index\n",
    "    \n",
    "    remove_index =[]\n",
    "    for col in df.columns:\n",
    "        remove_index.extend(outliers(df, col))\n",
    "        \n",
    "    remove_index=sorted(set(remove_index))    \n",
    "    df = df.drop(remove_index)    \n",
    "    return df \n",
    "\n",
    "# The following section is for auto_visaulizatin DTreeReg_mix_gaussian_splits testing\n",
    "def get_gmm_splits(data, variable):     \n",
    "    \"\"\"\n",
    "    Function that splits the data base on the GMM algo. \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed to get peaks\n",
    "    variable: str - the variable that for peak detection\n",
    "    \"\"\" \n",
    "        \n",
    "    # extract variable data \n",
    "    data = np.array(data[variable])\n",
    "    # select the best n_numbers, looping from 1 to 3 >> edge case that max(data) == min (data), which means the variable is consant, then the best_n_components is defaluted to be 1 culster only.\n",
    "    #compare with n_components best fit the data, ranging from 1 to 3.\n",
    "    # based on the matric of gmm.bic Bayesian information criterion (BIC):This criterion gives us an estimation on how much is good the GMM in terms of predicting the data we actually have. The lower is the BIC, the better is the model to actually predict the data we have, and by extension, the true, unknown, distribution.\n",
    "\n",
    "    if max (data) == min(data):\n",
    "        best_n_components = 1\n",
    "    else: \n",
    "        gmm_result = []\n",
    "        for n_components in range(1,4):\n",
    "            gmm = GaussianMixture(n_components).fit(data.reshape(-1, 1))\n",
    "            gmm_result.append(gmm.bic(data.reshape(-1, 1)))\n",
    "        best_n_components = gmm_result.index(min(gmm_result))+1\n",
    "    \n",
    "    # use the selected the best n_numbers to split the data base on GMM algo\n",
    "    gmm = GaussianMixture(best_n_components).fit(data.reshape(-1, 1))\n",
    "    \n",
    "    # if there is only one Gaussian distribution, the split will just take the mean of distribution \n",
    "    if best_n_components == 1:\n",
    "        gmm_splits = gmm.means_[0] \n",
    "    else:\n",
    "    # if there is more than two Gaussian distributions, the splits will take the mean and weight and different distribution \n",
    "        gmm_df = pd.DataFrame({'weight':gmm.weights_,'means':gmm.means_.reshape(best_n_components,)})\n",
    "        gmm_df = gmm_df. sort_values(by='means').reset_index(drop=True)\n",
    "\n",
    "        for i in range(len(gmm_df)-1):\n",
    "        # the split will consider the means and weight of different distributions \n",
    "            gmm_df.loc[i, 'splits'] = gmm_df.loc[i, 'means'] + (gmm_df.loc[i+1, 'means'] -gmm_df.loc[i, 'means'])*gmm_df.loc[i, 'weight']/(gmm_df.loc[i, 'weight']+gmm_df.loc[i+1, 'weight'])\n",
    "        \n",
    "        gmm_splits = list(gmm_df['splits'].dropna().values)\n",
    "    # Return the gmm_splits    \n",
    "    return gmm_splits  \n",
    "\n",
    "def get_gmm_splits_number (data, variable): \n",
    "    \n",
    "    \"\"\"\n",
    "    Function that splits the data base on the GMM algo and return the number of gaussian distributions.  \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed to get peaks\n",
    "    variable: str - the variable that for peak detection\n",
    "    \"\"\"\n",
    "    # extract key_feature data \n",
    "    data = np.array(data[variable])\n",
    "    \n",
    "    # select the best n_numbers, looping from 1 to 3 >> edge case that max(data) == min (data), which means the variable is consant, then the gmm_splits_number is defaluted to be 1 culster only. \n",
    "    #compare with n_components best fit the data, ranging from 1 to 3.\n",
    "    # based on the matric of gmm.bic Bayesian information criterion (BIC):This criterion gives us an estimation on how much is good the GMM in terms of predicting the data we actually have. The lower is the BIC, the better is the model to actually predict the data we have, and by extension, the true, unknown, distribution.\n",
    "    \n",
    "    if max (data) == min(data):\n",
    "        gmm_splits_number = 1\n",
    "    else: \n",
    "        gmm_result = []\n",
    "        for n_components in range(1,4):\n",
    "            gmm = GaussianMixture(n_components).fit(data.reshape(-1, 1))\n",
    "            gmm_result.append(gmm.bic(data.reshape(-1, 1)))\n",
    "        gmm_splits_number = gmm_result.index(min(gmm_result))+1\n",
    "    #return the number of gaussian distributions.\n",
    "    return gmm_splits_number\n",
    "\n",
    "def DTreeReg_gmm_splits(data, target, variable):\n",
    "    \"\"\"\n",
    "    Function that return tree spltis \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed for the algo. \n",
    "    target: str - the 'dependent' variable in terms of unsupervised learning. \n",
    "    variable: str - the 'independent' variable in terms of unsupervised learning. \n",
    "    \"\"\"\n",
    "    \n",
    "    # DTreeReg_gmm_splits logic: \n",
    "    # if there are only ONE gaussian (gmm_splits_number ==1)detected, use unsupervised Tree Regression to split the dependent variable into two sets with max_depth =1\n",
    "    # if there are none or more than 2 gaussian detected, use unsupervised Tree Regression to split the dependent variable into more than two sets with max_depth =2 \n",
    "    \n",
    "    gmm_splits_number = get_gmm_splits_number(data, variable)\n",
    "    \n",
    "    if gmm_splits_number ==1:\n",
    "        DTreeReg = DecisionTreeRegressor(max_depth =1, min_samples_split = .2)\n",
    "    else:\n",
    "        DTreeReg = DecisionTreeRegressor(max_depth =2, min_samples_split = .2)        \n",
    "        \n",
    "    fit = DTreeReg.fit(data[[variable]],data[target])\n",
    "    tree_splits = np.sort(DTreeReg.tree_.threshold[(DTreeReg.tree_.threshold >= 0)])\n",
    "    \n",
    "    return tree_splits\n",
    "\n",
    "# The following section is for auto_visaulizatin DTreeReg_peak_splits testing\n",
    "def get_peaks(data, variable):    \n",
    "    \"\"\"\n",
    "    Function that return peaks\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed to get peaks\n",
    "    variable: str - the variable that for peak detection\n",
    "    \"\"\"\n",
    "    # create histograme based on the input key_feature\n",
    "    hist, bin_edges = np.histogram(data[variable],10)    \n",
    "    bin_edges = bin_edges[1:]\n",
    "    \n",
    "    # Length of the available\n",
    "    Length = len(data[variable])\n",
    " \n",
    "    # peaks detection logic: \n",
    "    # 1) divide the dataset into 10 bins \n",
    "    # 2) for any bin that has more than 10% of dataset, is defined as peak (return as index) \n",
    "    # 3) return the first element of the peak_bin element as the peak     \n",
    "    peaks, _ = find_peaks(hist, height=(Length*0.1,Length))\n",
    "    \n",
    "    # if there is peaks detected, return all the peaks, if none, return none\n",
    "    if len(peaks)>0:        \n",
    "        peaks = bin_edges[peaks]\n",
    "    else:\n",
    "        peaks = []\n",
    "    return peaks\n",
    "\n",
    "def DTreeReg_peak_splits(data,target,variable):    \n",
    "    \"\"\"\n",
    "    Function that return tree spltis \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed for the algo. \n",
    "    target: str - the 'dependent' variable in terms of unsupervised learning. \n",
    "    variable: str - the 'independent' variable in terms of unsupervised learning. \n",
    "    \"\"\"\n",
    "    # call get_peaks function to get peaks detected from the independent variable\n",
    "    peaks = get_peaks(data,variable)     \n",
    "    peaks_number = len(peaks)\n",
    "    \n",
    "    # DTreeReg_peak_splits logic: \n",
    "    # if there are only ONE peak (peaks_number ==1)detected from get_peaks, use unsupervised Tree Regression to split the dependent variable into two sets with max_depth =1\n",
    "    # if there are none or more than 2 peaks detected, use unsupervised Tree Regression to split the dependent variable into more than two sets with max_depth =2 \n",
    "           \n",
    "    if peaks_number ==1:\n",
    "        DTreeReg = DecisionTreeRegressor(max_depth =1, min_samples_split = .2)\n",
    "    else:\n",
    "        DTreeReg = DecisionTreeRegressor(max_depth =2, min_samples_split = .2)\n",
    "    # fit the Tree regression data    \n",
    "    fit = DTreeReg.fit(data[[variable]],data[target])\n",
    "    tree_splits = np.sort(DTreeReg.tree_.threshold[(DTreeReg.tree_.threshold >= 0)])    \n",
    "    return tree_splits\n",
    "\n",
    "# The following section is for auto_visaulizatin Plotting\n",
    "\n",
    "def BW_plot (data,target,variable,tree_splits):\n",
    "    \"\"\"\n",
    "    Function that return tree spltis \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed for the algo. \n",
    "    target: str - the 'dependent' variable in terms of unsupervised learning. \n",
    "    variable: str - the 'independent' variable in terms of unsupervised learning.\n",
    "    tree_splits: list - the way the unsupervised tree algo. split the 'independent' variable.\n",
    "    \"\"\"\n",
    "    # create empty lists to store data    \n",
    "    conditions = [None]*(len(tree_splits)+1)\n",
    "    cats = [None]*(len(tree_splits)+1)    \n",
    "    #create a new dataframe for target and key_feature only \n",
    "    visual_df = data[[target,variable]]\n",
    "    \n",
    "    # BW splits logic: \n",
    "    # 1) If there is only one tree_splits (len(tree_splits)==1), split the data into two category base on the tree_splits\n",
    "    # 2) If there is more than one tree_splits, split the data into multiple datasets base on the tree_splits\n",
    "    \n",
    "    # Helper function to detect whether certain category is outlier with total datapoint less than 10%\n",
    "    # Reason, becasue the minimum split of trees is 10% \n",
    "    def outlier_detect(visual_df, condition):\n",
    "        percentage = len(visual_df[condition])/len(visual_df)\n",
    "        if percentage<0.1:\n",
    "            outlier = ' (Outlier)'\n",
    "        else: \n",
    "            outlier=''\n",
    "        return outlier \n",
    "    \n",
    "    if len(tree_splits)==1:        \n",
    "        conditions[0] = (visual_df[variable] < tree_splits[0])\n",
    "        cats[0] = '<'+str(round(tree_splits[0],2))\n",
    "        conditions[1] = (visual_df[variable] >= tree_splits[0])\n",
    "        cats[1] = '>='+str(round(tree_splits[0],2))    \n",
    "    else:    \n",
    "\n",
    "        for i in range(len(tree_splits)+1):\n",
    "            if i == 0:                \n",
    "                condition = visual_df[variable] < tree_splits[i] \n",
    "                #print(condition)\n",
    "                conditions[i] = (condition)\n",
    "                outlier = outlier_detect(visual_df, condition)\n",
    "                cats[i] = '<'+str(round(tree_splits[i],2))+ outlier \n",
    "            elif i == len(tree_splits):                \n",
    "                condition = visual_df[variable] > tree_splits[-1]\n",
    "                conditions[i] = (condition)\n",
    "                outlier = outlier_detect(visual_df, condition)\n",
    "                cats[i]  = '>'+str(round(tree_splits[-1],2))+outlier                 \n",
    "            else:\n",
    "                condition = (visual_df[variable] < tree_splits[i]) & (visual_df[variable] >= tree_splits[i-1])\n",
    "                conditions[i] = (condition)\n",
    "                outlier = outlier_detect(visual_df, condition)\n",
    "                cats[i] = str(round(tree_splits[i-1],2))+'-'+str(round(tree_splits[i],2))+outlier\n",
    "                \n",
    "    # conditions_result is used to categoraize targets based on cats setting\n",
    "    conditions_result = np.select(conditions, cats)\n",
    "    visual_df.insert(2,'Category',conditions_result) \n",
    "    \n",
    "    ax = sns.boxplot(x='Category', y= target, data=visual_df, order = cats).set(title=variable)     \n",
    "    # calculate and print out the average of each category \n",
    "    avg_df = visual_df.groupby('Category')[target].mean().reindex(cats).reset_index()\n",
    "    print(avg_df)\n",
    "    \n",
    "def feature_distribution_splits(data,feature,splits):\n",
    "    \"\"\"\n",
    "    Function that plot the histogram of interested feature with splits or peaks return from other function. \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed for the algo. \n",
    "    feature: str - the interested feature to plot as histogram\n",
    "    splits: list - the splits / peaks need to be visualized from the histogram graph. \n",
    "    \"\"\"    \n",
    "    plt.figure()\n",
    "    data[feature].hist(bins=100)\n",
    "    \n",
    "    for xc in splits:\n",
    "        plt.axvline(x=xc,color='r',ls='--')\n",
    "        \n",
    "    plt.title(feature) \n",
    "    plt.show()\n",
    "    \n",
    "def feature_distribution(data,feature):\n",
    "    \"\"\"\n",
    "    Function that plot the histogram of interested feature. \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data needed for the algo. \n",
    "    feature: str - the interested feature to plot as histogram \n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    data[feature].hist(bins=100)            \n",
    "    plt.title(feature) \n",
    "    plt.show()\n",
    "\n",
    "def time_series_plot (data,feature):    \n",
    "    \"\"\"\n",
    "    Function that plot the time-series plot of interested feature with rolling average of the last 4 hours\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data for plotting. \n",
    "    feature: str - the interested feature to plot  the time-series\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data[[feature,'time_bucket_local']])\n",
    "    new_col_name = feature + ' moving average (24hr)'\n",
    "    df[new_col_name] = df[feature].rolling(48).mean()\n",
    "    \n",
    "    #Addressing overlare axis issues pt1\n",
    "    df['time_bucket_local'] = pd.to_datetime(df['time_bucket_local'])\n",
    "   #n = len(df['time_bucket_local']) // min(int(n_month),8)\n",
    "    n = len(df['time_bucket_local']) // 8\n",
    "\n",
    "    # set figure size\n",
    "    plt.figure( figsize = ( 12, 5))\n",
    "    \n",
    "    # plot a simple time series plot\n",
    "    # using seaborn.lineplot()\n",
    "    sns.lineplot(x = 'time_bucket_local',\n",
    "             y = feature,\n",
    "             data = df,\n",
    "             label =feature)\n",
    "  \n",
    "    # plot using rolling average\n",
    "    sns.lineplot(x = 'time_bucket_local', \n",
    "                 y = new_col_name,\n",
    "                 data = df,\n",
    "                 label = 'Average (24hrs)')\n",
    "    #Creating readable axis part 2\n",
    "    plt.xticks(df['time_bucket_local'][::n], rotation=45)\n",
    "\n",
    "\n",
    "# The following section is for machine leanring model selection, feature importance calculating\n",
    "def ranking_models(data,target,variables,model_dict):\n",
    "    \"\"\"\n",
    "    Function that select & return the most accurated ML that could capture the relationship beween the dependent vs. independent variables. \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data for ML training. \n",
    "    target: str - dependent variable (y)\n",
    "    variables: list - list of independent variables (X)\n",
    "    model_dict: dictinary - dictionary of sets of machine leanring model candidates    \n",
    "    \"\"\"\n",
    "    X = data[variables]\n",
    "    y=  data[target]\n",
    "    x_train, x_test, y_train, y_test=train_test_split(X,y,test_size=0.2)\n",
    "    models =[item for item in model_dict.items()]    \n",
    "    mape = []\n",
    "    mse=[]\n",
    "    names =[]     \n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=5, random_state=7,shuffle=True)\n",
    "        mape_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold,scoring = 'neg_mean_absolute_percentage_error')\n",
    "        mse_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold,scoring = 'neg_root_mean_squared_error')\n",
    "        mape.append(mape_results.mean())\n",
    "        mse.append(mse_results.mean())\n",
    "        names.append(name)\n",
    "    \n",
    "    mape = list(np.array(mape)*(-1)*100)\n",
    "    mse = list(np.array(mse)*(-1))\n",
    "    \n",
    "    #accuracy_df = pd.DataFrame({'model':names, 'abs_mape_%':mape,'cv':cv})\n",
    "    accuracy_df = pd.DataFrame({'model':names, 'abs_mape_%':mape,'abs_mse':mse})\n",
    "    #abs_mape the lower the better\n",
    "    accuracy_df  = accuracy_df.sort_values(by='abs_mape_%',ascending=True)\n",
    "    \n",
    "    selected_model = accuracy_df.iloc[0].model\n",
    "    \n",
    "    #print('Selected model is: '+ selected_model)\n",
    "    #print()\n",
    "   # print('Please wait for the modelling results...')\n",
    "    \n",
    "    return selected_model\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Function that normalize the data for linear regression based model \n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data for ML training. \n",
    "    \"\"\"       \n",
    "    normalized_data = data.copy()\n",
    "    for feature_name in normalized_data.columns:\n",
    "        max_value = normalized_data[feature_name].max()\n",
    "        min_value = normalized_data[feature_name].min()\n",
    "        normalized_data[feature_name] = (normalized_data[feature_name] - min_value) / (max_value - min_value)\n",
    "   \n",
    "    normalized_data = normalized_data.dropna(how='all',axis='columns')\n",
    "    return normalized_data\n",
    "\n",
    "def lasso_regression_feature_importance(data, target, variables):\n",
    "    \"\"\"\n",
    "    Function that calculate the feature importances based on the k-fold lasso_regression\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data for ML training. \n",
    "    target: str - dependent variable (y)\n",
    "    variables: list - list of independent variables (X)\n",
    "    \"\"\"\n",
    "    # normalized data \n",
    "    normalized_data = normalize(data)\n",
    "    # remaining featues \n",
    "    remaining_features = normalized_data.columns.to_list()\n",
    "    remaining_features.remove(target)    \n",
    "    \n",
    "    # Retrieve the target and independent variables \n",
    "    y= normalized_data[target]\n",
    "    X= normalized_data[remaining_features]\n",
    "    \n",
    "        \n",
    "    kf = KFold(n_splits=10)\n",
    "    lr = Lasso(alpha=0.01)\n",
    "    \n",
    "    # Calculate the feature importance\n",
    "    feature_importance_dict={}\n",
    "    count = 1     \n",
    "    for train, test in kf.split(X,y):\n",
    "        lr.fit(X.iloc[train], y.iloc[train])\n",
    "        lr_coef_list=lr.coef_.tolist()\n",
    "        feature_importance=pd.DataFrame({'feature':remaining_features,'coefficient':lr_coef_list})\n",
    "        feature_importance['feature_importance']=abs(feature_importance['coefficient'])\n",
    "        feature_importance_dict[count]=feature_importance[['feature_importance','feature']]\n",
    "        count += 1\n",
    "    feature_importance_all=pd.concat(feature_importance_dict.values())\n",
    "    \n",
    "    feature_importance_all =feature_importance_all.groupby('feature').mean().sort_values('feature_importance',ascending= False).reset_index()\n",
    "    \n",
    "    feature_importance_all['index'] = -feature_importance_all['feature_importance']\n",
    "    \n",
    "    return feature_importance_all\n",
    "\n",
    "def ridge_regression_feature_importance(data, target, variables):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that calculate the feature importances based on the k-fold ridge_regression\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data for ML training. \n",
    "    target: str - dependent variable (y)\n",
    "    variables: list - list of independent variables (X)\n",
    "    \"\"\"\n",
    "    # normalized data \n",
    "    normalized_data = normalize(data)\n",
    "    # remaining featues \n",
    "    remaining_features = normalized_data.columns.to_list()\n",
    "    remaining_features.remove(target)    \n",
    "    \n",
    "    # Retrieve the target and independent variables \n",
    "    y= normalized_data[target]\n",
    "    X= normalized_data[remaining_features]\n",
    "    \n",
    "        \n",
    "    kf = KFold(n_splits=10)\n",
    "    rr = Ridge(alpha=0.01)\n",
    "    \n",
    "    # Calculate the feature importance\n",
    "    feature_importance_dict={}\n",
    "    count = 1     \n",
    "    for train, test in kf.split(X,y):\n",
    "        rr.fit(X.iloc[train], y.iloc[train])\n",
    "        rr_coef_list=rr.coef_.tolist()\n",
    "        feature_importance=pd.DataFrame({'feature':remaining_features,'coefficient':rr_coef_list})\n",
    "        feature_importance['feature_importance']=abs(feature_importance['coefficient'])\n",
    "        feature_importance_dict[count]=feature_importance[['feature_importance','feature']]\n",
    "        count += 1\n",
    "    feature_importance_all=pd.concat(feature_importance_dict.values())\n",
    "    \n",
    "    feature_importance_all =feature_importance_all.groupby('feature').mean().sort_values('feature_importance',ascending= False).reset_index()\n",
    "    \n",
    "    feature_importance_all['index'] = -feature_importance_all['feature_importance']\n",
    "    \n",
    "    return feature_importance_all\n",
    "\n",
    "def linear_regression_feature_importance(data, target, variables):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that calculate the feature importances based on the k-fold linear_regression\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data for ML training. \n",
    "    target: str - dependent variable (y)\n",
    "    variables: list - list of independent variables (X)\n",
    "    \"\"\"\n",
    "    # normalized data \n",
    "    normalized_data = normalize(data)\n",
    "    # remaining featues \n",
    "    remaining_features = normalized_data.columns.to_list()\n",
    "    remaining_features.remove(target)    \n",
    "    \n",
    "    # Retrieve the target and independent variables \n",
    "    y= normalized_data[target]\n",
    "    X= normalized_data[remaining_features]\n",
    "    \n",
    "        \n",
    "    kf = KFold(n_splits=10)\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    # Calculate the feature importance\n",
    "    feature_importance_dict={}\n",
    "    count = 1     \n",
    "    for train, test in kf.split(X,y):\n",
    "        lr.fit(X.iloc[train], y.iloc[train])\n",
    "        lr_coef_list=lr.coef_.tolist()\n",
    "        feature_importance=pd.DataFrame({'feature':remaining_features,'coefficient':lr_coef_list})\n",
    "        feature_importance['feature_importance']=abs(feature_importance['coefficient'])\n",
    "        feature_importance_dict[count]=feature_importance[['feature_importance','feature']]\n",
    "        count += 1\n",
    "    feature_importance_all=pd.concat(feature_importance_dict.values())\n",
    "    \n",
    "    feature_importance_all =feature_importance_all.groupby('feature').mean().sort_values('feature_importance',ascending= False).reset_index()\n",
    "    \n",
    "    feature_importance_all['index'] = -feature_importance_all['feature_importance']\n",
    "    \n",
    "    return feature_importance_all\n",
    "\n",
    "def random_forest_feature_selection(data, target, variables):\n",
    "    \"\"\"\n",
    "    Function that calculate the feature importances based on the k-fold random forest regression\n",
    "    Parameters:\n",
    "    data: pd.DataFrame - dataframe containing the data for ML training. \n",
    "    target: str - dependent variable (y)\n",
    "    variables: list - list of independent variables (X)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = data[variables]\n",
    "    y=  data[target]\n",
    "    kf = KFold(n_splits=10)\n",
    "    rf = RandomForestRegressor(n_estimators=50) \n",
    "    \n",
    "    # Calculate the feature importance\n",
    "    feature_importance_dict={}\n",
    "    count = 1\n",
    "    for train, test in kf.split(X, y):\n",
    "        rf.fit(X.iloc[train], y.iloc[train])\n",
    "        feature_importance = pd.DataFrame(variables,rf.feature_importances_).reset_index()   \n",
    "        feature_importance =feature_importance.sort_values('index',ascending=False).reset_index().rename({'index':'feature_importance', 0:'feature'},axis=1).reset_index()  \n",
    "        feature_importance_dict[count]=feature_importance[['index','feature_importance','feature']]\n",
    "        count += 1\n",
    "    feature_importance_all=pd.concat(feature_importance_dict.values())\n",
    "    feature_importance_all =feature_importance_all.groupby('feature').mean().sort_values('index',ascending=True).reset_index()\n",
    "    return feature_importance_all\n",
    "\n",
    "def feature_importance (feature_importance_all):    \n",
    "    #print(feature_importance_all)\n",
    "    #plt.figure( figsize = ( 12, 5))\n",
    "    feature_importance_allt10 = feature_importance_all.head(10)\n",
    "    #changing to all10 to limit to top 10\n",
    "    max_index = max(feature_importance_allt10['index'])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.suptitle('Feature Importance')\n",
    "    plt.barh(feature_importance_allt10['feature'],max_index-feature_importance_allt10['index'])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Key Drivers functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
